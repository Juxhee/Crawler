{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orbi_url_crawler(keyword,page_num):\n",
    "    url_list = []\n",
    "    date_list = []\n",
    "    for page in range(page_num+1):\n",
    "        url = f'https://orbi.kr/search?q={keyword}&page={page}'\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source, 'lxml')\n",
    "        body = soup.find_all(\"div\",class_='body')\n",
    "        for i in body:\n",
    "            text_list = i.find_all('div',class_='list-text')\n",
    "            author_list = i.find_all('div',class_='author-text')\n",
    "            for text in author_list:\n",
    "                date = text.find('abbr').text\n",
    "                date_list.append(date)\n",
    "            for text in text_list:\n",
    "                href = text.find_all('a')\n",
    "                for h in href:\n",
    "                    url = h.attrs['href']\n",
    "                    url = 'http://orbi.kr/' + url\n",
    "                    url_list.append(url)\n",
    "                \n",
    "        time.sleep(3)\n",
    "        driver.close()\n",
    "    #drop_date = [index for index, date in enumerate(date_list) if date <= '19/01/01 00:00']\n",
    "    #new_date = np.delete(date_list, drop_date, axis=0)\n",
    "    #new_url = np.delete(url_list, drop_date, axis=0)\n",
    "    df = pd.DataFrame({'url':url_list,'date':date_list})\n",
    "    return df\n",
    "\n",
    "keyword = \"산업공학과\"\n",
    " \n",
    "final_url = orbi_url_crawler(keyword,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orbi_crawler(final_url_list, final_date_list)\n",
    "    final_content = pd.DataFrame(columns = ['title','content','date'])\n",
    "    final_comment = pd.DataFrame(columns = ['comment','date'])\n",
    "    for i in range(len(final_url_list)):\n",
    "        res = requests.get(final_url_list[i])\n",
    "        soup = BeautifulSoup(res.text, \"lxml\")\n",
    "        comment_list = []\n",
    "        date_list = []\n",
    "        df1 = pd.DataFrame()\n",
    "        df2 = pd.DataFrame()\n",
    "        title_list = []\n",
    "        content_list = []\n",
    "        comment_date_list = []\n",
    "        title = soup.find('h1',class_='title').text\n",
    "        content = soup.find('div',class_='content-wrap').text\n",
    "        title_list.append(title)\n",
    "        content_list.append(content)\n",
    "        date_list.append(final_date_list[i])\n",
    "        comment_box = soup.find_all('div',class_='comment-content')\n",
    "        for comment in comment_box:\n",
    "            text = comment.find('div',class_='comment-text').text\n",
    "            #print(text)\n",
    "            comment_list.append(text)\n",
    "            comment_date_list.append(final_date_list[i])\n",
    "        #comment_df = sum(comment_list, [])\n",
    "        df1 = pd.DataFrame({'title':title_list,'content':content_list,'date':date_list})\n",
    "        df2 = pd.DataFrame({'comment':comment_list, 'date':comment_date_list})\n",
    "        #df['url'] = url\n",
    "        final_content = pd.concat([final_content, df1])\n",
    "        final_comment = pd.concat([final_comment, df2])\n",
    "        time.sleep(3)\n",
    "    return final_content, final_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
